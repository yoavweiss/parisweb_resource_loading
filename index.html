<!DOCTYPE html><html lang=en><head><meta http-equiv=Content-Type content="text/html; charset=UTF-8"><meta name=viewport content="width=device-width,initial-scale=1"><link rel="stylesheet" href="slidestyles.css" type="text/css" media="screen" charset="utf-8">

<!-- #ToDo: Change Title for the presentation -->
<title>Why is the web slow?</title>

</head>
    
<!-- #ToDo: Change 16:9 to 4:3 or vice versa, according to the current projector / screen -->
<body onload="var slideshow = remark.create({ratio: '16:9', navigation: {scroll: true, touch: true, click: false}, countIncrementalSlides: false, highlightLanguage: 'bash', highlightStyle: 'tomorrow-night-blue'});">

<!-- #ToDo: Write your slides in MarkDown here -->
<!--
# &#x276e;link&#x276F;
-->

<textarea id="source">

layout: true
class: center, middle
background-image: url(media/fast.jpg)

.low-right[@yoavweiss]
---

### Faster Bytes are not Enough!!
### Why is the web slow?
### (& what can we do about it)

<br>
<br>
.center[Yoav Weiss&nbsp;&nbsp;|&nbsp;&nbsp;   @yoavweiss]
.right[![](media/Akamai-Logo-RGB.png)]

???

Hi, I'm Yoav Weiss. I work at Akamai on making our CDN as well as browsers faster, and I'm here today to
talk about why the Web is inherently slow (which is a sad thing to say in a performance conference), and how can we make it faster despite that fact.
---
background-image: url(media/pc.jpg)

???
I’ve been in this industry for a while now. When I first started in the industry, I was working on a
web optimization proxy, as one does. My very first task was to build a
proprietary SPDY-like client-server protocol in order to be able to send
multiple requests and receive multiple responses at once over a single
connection. My second task was to integrate an image compression library
into the server. The image compression server was usually getting 2x
size reduction on the images. And network bandwidth at the time was a
serious issue, with top-notch mobile networks clocking at a cool
33kbps!! 
---
background-image: url(media/surprise.jpg)

???
But, we soon noticed something odd. Even with those awful
speeds, a 2x reduction in image sizes did not give us similar
performance improvements, unless they were deployed at the same time as
the SPDY-like client-server protocol (which could only be deployed for
users that actively installed the client, which was very few of them).
At first we were baffled by this. If we’re cutting down the
bytes we’re sending down by a factor of 2, why aren’t we seeing 2x
performance improvements???

---

# `time != `
# `(bytes / BW)`

???
The reason this was the case was that on the Web, the basic equations to calculate
how long "downloading a site" would take do not apply. We cannot treat the contents of a site as "bytes".
The contents of a Web site are discovered progressively, rendered progressively and often continue to download beyond the point in which
considers the site to be "loaded".
---

# Latency!

???
One of the reasons for the non-linearity that we saw was the impact of latency on the HTTP protocol.
The fancy networks at the time only delivered 33Kbps, but they also
did that with round trip times of 700ms, making the page’s loading
performance dominated by latency.

---

# HTTP/1.1 `¯\_(ツ)_/¯`
???
Requests in HTTP/1.0 and HTTP/1.1
were going out over multiple connections (2 connections per host for 1.1
at the time), but each connection could handle a single request at a
time, which meant that if you have to bring in 50 images to the page, it
would take you at a minimum 25 RTTs (even if all your images fit in a
single packet), or 17.5 seconds!!
---
class: bright

![](media/http1_1.svg)

---
class: bright

![](media/http1_2.svg)
---
class: bright

![](media/http1_3.svg)

???

But, that was a long time ago, right?
The industry has moved on, and those problems are all gone, *right*?

Well, we have advanced a lot in the last… almost 20 years. I’m really
old...
---

# Better *everything*!

???
We have significantly better networks, newer protocols such as HTTP2 and
QUIC, we have improved image formats, and we have significantly faster
computers.
Latency of cellular networks have decreased significantly (even when physical latency because you're fetching something from the other end
of the globe)
But we also have slower computers (pull out phone)

---
![](media/js_transfer_size.png)

???

And content on the web has evolved significantly over the last 20 years.
From simple text sites with images and an occasional JS based form
validation, it turned into a full fledged application development
platform, and one where you *download your entire app every time you
want to use it*.

---
.center[![](media/weird.jpg)]
???

And as a result the Web is still weird. Our performance bottlenecks are rarely where we think they are.
And despite high speed networks, we still see a *lot* of slow websites out there.

I'm of a firm belief that in order to understand how to make something work better and faster, you have to understand how it works in the
first place. And as a browser engineer, I really think everyone working on the web should know more about how browsers work.

---

# We'll discuss:
.center.big[
* How browsers work
* Web Performance issues
* Solutions
]

???

(6 minutes)

---
class: contain
background-image: url(media/dictionary.jpg)

???

But first things first, let’s start with some definitions.
Some people consider the words “web” and “internet” interchangeable. But
they have very distinct meanings.

---
class: contain
background-image: url(media/what_is_the_internet.png)

???

As our friends at Google tell us, the internet is a global computer
network providing a variety of information and communication facilities,
consisting of interconnected networks using standardized communication
protocols.

---

class: contain
background-image: url(media/what_is_the_web.png)

???
The web however, is “a network of fine threads constructed by a spider
from flui..” no
---
class: contain
background-image: url(media/world_wide_web.png)

???

The web is a network of interlinked documents, identified by their URLs,
and accessible via the internet. In other words, it’s a network of
documents, applications and content that is delivered over a network of
computers, the internet.

---
class: contain
background-image: url(media/what_is_a_browser.png)

???

A browser is a computer program that is used to display HTML content in order
to navigate the web.

---
.center[![](media/networks.jpg)]

???

So if we’d take the route of a metaphor, the internet is the roads, the
web is a network of interconnected attractions on those roads, URLs is
the road signs (stretching it, I know) and the browser is the vehicle we
can use to move from one location to the next.
---
class: bright

.inline[![](media/chrome.png)
![](media/safari.png)
![](media/firefox.png)
![](media/msedge.svg)
![](media/samsung.png)
]
???
So how do browsers load web pages?

---

##What's in a browser?
.center.big[
* Rendering engine
* Javascript engine
* Graphics layer
* Networking layer
* Image, video & audio decoders
]

???

Browsers are built out of multiple components that all play a part in loading of web pages: The networking layer fetches the required
resources from the web server, the rendering engine interprets them and finds more required resources, the javascript engine executes the
scripts which may often lead to more resources that needs to load, the decoders decode media formats into something that can be displayed to
the user and the graphics layer paints it all on the user's screen. It's a delicate and complex dance.
---
![](media/example.png)

???

So what happens when a user clicks on a link or types in an address in their browser's URL bar?

---
class: bright
![](media/dns.svg)

???

1 RTT for DNS
---

class: bright
![](media/dnstcp.svg)

???
Another RTT for TCP
---
class: bright
![](media/dnstcptls.svg)

???
2 more RTTs for TLS

---
.center[![](media/tcp_slow_start.png)]

???
TCP slow start is a mechanism that enables the server to gradually discover the connection's limits.
It also limits us in the amount of data that we can initially send.

---
class: bright
#Preloader
![](media/tokenization.svg)

???

Once the server actually sends the HTML down to the browser, the browser starts processing it.
It tokenizes the HTML, and runs it through its preloader, in order to find out ASAP which resources it needs to load.
---

#Resource loading

???

That's because a web page is built out of multiple different file types: HTML, CSS, JS, fonts, images and  other media formats.
These resources are discovered by the preload scanner, by calculating style and by executing JS.
And the browser needs to discover those resources ASAP, because some of them are critical to render the page.
---
![](media/critical_rendering_path.png)

???
The browser needs the HTML in order to create the DOM - the Document Object Model.
Blocking scripts block the parsing of HTML until they are downloaded, so they block the DOM creation.
And CSS is required in order to create the CSS Object Model which together with the DOM is required to create the render tree, which is
required for page layout and paint to happen.

So before the user sees *anything* we need to download enough of the HTML and all the blocking CSS and JS.
---

# Priority

### HTML > CSS > JS > Fonts > Images

???
When downloading resources browsers need to decide which resources are more equal than others.
The basics is described here: HTML > CSS > JS > Fonts > Images, but in practice implementations have much more complex logic. Blocking scripts at the top are higher priority than
blocking scripts at the bottom. Async scripts are lower priority than blocking scripts. In-viewport images are higher priority than
out-of-viewport images. And this entire logic can vary between browsers and between browser versions.
---

![](media/tree_html_only.svg)

???
So, if we look at our resource download dependency tree, the browser starts out by just downloading the HTML. That's the only URL it's aware of.

---

![](media/tree_html_first_subresources.svg)

???

Then as the HTML is processed, CSS, images and scripts are discovered.
---

![](media/tree_html_second_subresources.svg)

???

But in many cases, CSS and scripts load other resources.
---

![](media/tree_html_third_subresources.svg)

???

Which then can load even more resources


CSS background images are declared in CSS, so discoverable only when the browser calculates the style.
Fonts were later added to the mix, and fonts are even more complicated. As they're only downloaded when the browser knows that they will be
used. So it has to calculate the style, and cross those style with the DOM to see if the resource is really needed.

(13 minutes)
---

## Performance issues

.center.huge[
* Server side processing

* Discovery

* Bloat

* Out of control third parties

* Client side processing
]

???
Let's go over these problems in detail and see how we can reduce the
pain they cause.

---
background-image: url(media/servers.jpg)

# Server side processing

???

Historically, before the seminal web performance work by Steve Souders,
server side processing was considered *the* problem in web performance,
and it was what people were optimizing for. While Steve showed that
front end performance is responsible for 80% of the time people are
waiting for web pages, 20% is nothing to scoff at.
So server side performance is still very much something to be concerned
with.
At Akamai we see median server side performance times (or TTFB between the edge server and the origin server) of ~450ms, which
is a lot, especially if you consider the 3 seconds performance budget.

---
background-image: url(media/flush.jpg)
#Flush early

???
Early advice on server side performance had the advice to "flush early"
- you don't have to wait for your DB in order to let the browser start
  processing your most-probably-static HTML head and start fetching some
resources there. Only that this is often not the case - if your DB calls
can fail, they can impact your entire document (e.g. respond with a
completely different 404 page vs. your 200 page), and it's not always
trivial to "switch" pages on the user after an error has happened (and
without reloading the whole thing).

---

# Discovery
![](media/discovery.png)

???
Because of the way browsers work, because the browser has to download
the HTML, in order to find subresources to download, which in turn,
especially CSS and JS download more resources that they depend on, a lot
of the resources the browser needs to download are discovered at a late
phase.
Fonts and background images are notorious in that sense, since the
browser has to download all the CSS, and process the DOM in order to
find what's needed to be downloaded. But Javascript that loads resources
is just as guilty, as the entire script has to be processed before the
resource download can start.

---

background-image: url(media/dependency_tree.jpeg)
## Dependencies!

???
And this creates a dependency tree that's deep and contains many nodes,
each layer in that tree depends upon the downloading and processing of
the layer above it.
Now some resources are better at this than others: HTML and SVG can be
processed as they come, so a resource dependency in their first 100
bytes doesn't need to wait until the entire resource is downloaded
before the download can start.
But, CSS and JS are not the same. They have to be processed in their
entirety (and for CSS, all the blocking CSS has to be downloaed and
processed) before any resource download can start.
And that's not a bug, it's inherent to the way these formats work.

And there's inherent tension here between flattenning the dependency
tree which would help the browser load the page earlier, and the fact
that humans write websites, and for the humans, it's often better to
have smaller modules, which they can reason about without looking at the
larger picture.
So we have the old-fashioned CSS `@import`s and the newly fangled ES6 modules. They make
writing and managing code dependencies significantly easier. They also
make it harder for the browser, adding another layer to that dependency
tree.

---
background-image: url(media/bloat.jpg)

# Bloat

???

Traditionally we have been building web sites for desktops. 
Also traditinally, we compressed our images to avoid that kind of
experience
(autoplaying video of an image downloading slowly)
---
background-image: url(media/images_trend.png)

???
As broadband got better, we compressed our images less and less (HTTP
archive graph), and added more of them. And for our mobile version of
the site, we kept it slim.
But as more and more devices came to the market, each with its own
viewport dimensions, building a site version became simply not feasible. 
As a result, responsive design took over web design by storm, but in its
most naive version, we kept sending the same images we needed for our
high-end, high-bandwidth user experience to all devices.


---
background-image: url(media/devices.jpg)

# Client side processing

???

But the problem with bloat is not just the bandwidth waste. These
devices (points to phone) are not just downloading resources on an often
flakier network than our desktops. They also often have less CPU power
to process larger assets and less memory to deal with the consequences.

And even if we solved bloat for images, there's still a lot of CSS and
JS bloat to go around...
CSS frameworks are great to develop to, but often mean that you're
downloading way more CSS than you actually need *as a blocking
resource*.
JS frameworks and libraries often have a similar impact only that they
also tend to peg down the mobile device's CPU as their JS is being
processed (show timeline with CPU).


---
layout: true
class: center, middle
background-image: url(media/taking_back_control.jpg)

.low-right[@yoavweiss]
---

# Third parties

???
Everyone like to complain about third party performance...
It feels good to complain about that, cause by definition, it's someone
else that's doing a bad thing, and you're innocent :)

But when people talk about "third party performance problems" they usually conflate many different things.

---

## Render blocking

???

Some third parties are render blocking and will slow down your web site's first paint if they're slow to load.
---

## SPOF

???
Others, when loaded as blocking scripts or render critical CSS, can effectively DoS your site if they fail to load for some reason.
---

## Bandwidth contention with more critical resources

???
What often happens is that since third parties are download over separate H2 connections from different servers, their resources often
contend on bandwidth with your site's resources, and can slow them down, even if they are not blocking in and of themselves.
---

## Excessive downloads

???
Then there's the issue of badly behaving third parties which download excessive amounts of content which the user doesn't necessarily care
about.
---

## Periodic downloads

???
They often send periodic beacons which wake up the user's radio network and drain the user's battery.
---

## Scroll/touch performance

???
They hijack the user's scroll and touch events in order to keep tabs on their activity on the page, which can easily result in janky scroll
and touch user experience on your site.
---

## Hogging main thread

???
Finally, downloading massive amounts of scripts and images can easily cause main thread contention, where the browser is mostly executing
javascript and decodes images for third parties rather than render your content, or react to your users actions.

---
background-image: url(media/whose-line-is-it-anyway.jpg)

# Whose??

???

But maybe the hardest part about third parties is that as a developer, you don't always know what's running on your site.
You're adding in a Tag Manager, that's bringing in its tag friends, which bring their tag friends, and together they all have a third party
party, all over your web site. And you have very little say in the matter.


(21 minutes)
---

layout: true
class: center, middle
background-image: url(media/fast.jpg)

.low-right[@yoavweiss]
---
background-image: url(media/controls.jpg)
## What can we do about it?

???

Are we all properly depressed yet?
Everything is awful and slow.

So, how do we solve this? What can we do about it?
How can we make our websites fast despite all this awfulness?

aside: I'll briefly go over the solutions, as explaining them in depth can take a full presentation on its own. But feel free to grab me
later for any details you're missing. I'll also be happy to direct you to some reading material about them.

---
background-image: url(media/servers.jpg)
# Server Side processing

???
It's hard to give you generic advice on how to speed up your server side processing. You can instrument and figure out which operations take
time, and try to speed them up, but at the end of the day, executing complex, scalable applications to create dynamic HTML pages can take
time.

It also doesn't help that the default runtime of many high level languages such as Python and Ruby is not extremely fast be default.

But even if our backend times can be slow at times, that doesn't mean we can't put this waiting time to good use.
---
background-image: url(media/servers.jpg)
# H2 push

???

Server push is a feature of the HTTP/2 protocol, which enables us to send down resources before the browser requested them.
As such, it enables us to put the network connection to good use while we're waiting for the HTML to be generated by the server.
---

# Using the wait time

.center[![](media/espn_bandwidth_pre_html.png)]

???

In other words it enables us to turn this bandwidth graph, into this.

---

# Using the wait time

.center[![](media/espn_bandwidth_pre_html_potential.png)]
---
![](media/page_loading_nopush.svg)

???
300 ms think time
100 ms RTT
98KB CSS+JS
120KB HTML

---
![](media/page_loading_push.svg)

???

Another advantage of this is that it enables us to warn up the TCP connection while waiting for the HTML, which can significantly reduce the
amount of RTTs and time it takes us to send down the HTML and the critical CSS and JS resources.
---

# Discovery
![](media/discovery.png)

???

We've seen earlier that discovery is a real issue. The browser cannot download the resources it doesn't know about,
so it has to download resources in order to know what other resources to download, making the whole process highly dependent on latency.

What are the solutions to that problem?
---
background-image: url(media/discovery.png)
# Preconnect

## `<link rel=preconnect>`
???

A first step which can help, at least for blocking third parties is to preconnect to their host.
That can eliminate large parts of the latency required to fetch them: DNS, TCP and TLS, or at least take these parts out of the critical
path.

---
background-image: url(media/discovery.png)
# H2 push

???

We already discussed H2 push in the context of server side processing, but it can have a double impact here.
H2 push can be used to push critical resources before the browser
  discovers them and realizes that it needs them. And it can do that before the HTML arrived.
---
background-image: url(media/discovery.png)
# Preload

## `<link rel=preload>`
???

Preload is another recent feature that I've been working on. It enables you to tell the browser to kick off the download of a certain
resource, without defining the exact point where the resource would be used. So, it decouples download from execution, and enables you to
"flatten" the dependency tree by preloading late-discovered resources ahead of time.

---
background-image: url(media/discovery.png)

# Preload &gt; Push

.center.huge[
* Cross origin
* Cache + cookies
* Content negotiation
* Load/error events
]

???
If we compare preload to push, it has multiple advantages: it can work for third party resources where H2 push is essentially meant for
resources under the first party's control. It also takes the cache state into account, as well as the cookies on the client side, where push
cannot do that. It can be used in content-negotiation scenarios, so for example can be used to fetch webp images only to browsers that
supprot them, using `Accept` headers. On top of that, it has load and error events, enabling you to build complex JS based loading logic
using preload.

---
background-image: url(media/discovery.png)

# Preload &lt; Push

.center[![](media/espn_bandwidth_pre_html_potential.png)]
???

What's push biggest advantage? The one we've seen before: it can be used to "fill up" the idle times before the browser the browser received
the HTML, resulting in a significant boost to the time the user first sees content on the screen.

---
background-image: url(media/discovery.png)

.left[
```javascript
let img = new Image();
if (Math.random() > 0.5) {
    img.src = "image1.jpg";
} else {
    img.src = "image2.jpg";
}
```
]

???
Does push and preload solve all discoverability problems?
Not really, sometimes actual JS needs to run on the client in order to
figure out what resource the browser will need
(show code that randomally pick either img)

But as web developers, we can try to limit that variablity and make sure
we push and preload everything we can that's needed.

---
class: bottom
background-image: url(media/ted.jpg)

## Adaptive Acceleration

???
And because Ted made me explicitly say it: preconnect, H2 push are currently available as part of Adaptive Acceleration, and we're looking
into automatic preload.

(Ted's face as a background)

---
background-image: url(media/discovery.png)
# Early hints

???
A recent standard proposal also suggests to define a way for us to be able to send headers to the browser before the response is complete,
which can enable us to send instructions to the browser to preconnect hosts or preload resources before the response is ready, which can be
helpful to speed up blocking third parties, especially if they are lately discovered.
---
background-image: url(media/discovery.png)

# Priorities

##`<link rel=preload as=script>`

???
Just a short aside: preload enables the browser to assign the right priorities to the resource in question by declaring the type of resource
using the `as` attribute. I'm also working on a different specification that will enable us to define explicit priorities for certain
resources, and change their default priorities.

(29 minutes)
---
background-image: url(media/bloat.jpg)
# Bloat

???
What can we do about bloat? What can we do about the fact that we're sending too much content down?
Well, the good news here is that bloat is in our hands. We just need to send less bytes down the wire.
---
background-image: url(media/bloat.jpg)
# Image Compression

???

The first and obvious step against bloat is better image compression. That's often easier said than done.
While the lossless aspects of image compression are obvious to automate, as they don't impact the visible quality, the lossy parts are
harder, as you need to make sure you're compressing as much as you can without creating visible artifacts that will degrade the user
experience and will ultimately impact conversions (plus you'd have your designers on our back).

But there are multiple open source projects that you can integrate into your build process and can help you get better compression results.
---
background-image: url(media/bloat.jpg)
# Responsive images

## `<picture>`
## `srcset` & `sizes`
???

And then there's responsive images.
We talked before about the fact that responsive design initially resulted in sending high resolution images to all devices.

The web performance community realized that this is a huge issue, formed the Responsive Images Community
Group, and after a long while and numerous arguments, we managed to
convince browser vendors that we need to a way to specify multiple
images for browsers to download, based on their device dimensions,
design and the browser's environment.

And we came up with `<picture>`, srcset and sizes, specified them and
shipped them in browsers.

That, along with Client Hints, gives us the tools to manage image bloat
on the Web, and reduce it to a minimum by sending devices the images they need, and not images larger than that.

There's also much progress on bloat in terms of better compression
algorithms, better image formats, etc.

---
class: bottom
background-image: url(media/moe.jpg)

# Image Manager

???
(Moe's face as background)
The main hurdle to adoption with responsive images is that managing all those variants is hard work. And just in case it's not obvious,
Image Manager is *really* good at handling all that complexity for you.
---
background-image: url(media/bloat.jpg)
# Zopfli & Brotli

???
More on the front of bloat, new compression algorithms enable us to shrink our text files even further.
The Zopfli and Brotli algorithms enable us to better compress text files. Zopfli is using gzip compatible file, only investing a lot of
power in compressing them further, getting ~7% better compression on average. Brotli is a new format, supported by all modern browsers,
which gives 20-30% better compression for text files.

---
class: bottom
background-image: url(media/ted.jpg)

## Resource Optimizer

???
(Ted's face)

And again, just in case you don't know, Resource Optimizer, the latest addition to Adaptive Acceleration, performs automatic Brotli & Zopflli
compression for your static, cacheable resources. And if you prefer to serve Brotli from origin, we support that as well, and properly cache
the different variants.

---
background-image: url(media/bloat.jpg)
# H2 shared dictionaries

???
We talked before about dependencies, small modules and their discovery issues.
There's another aspect to that, as small modules tend to compress poorly. Traditionally we've bundled resources together to avoid the
negative impact of extra requests on HTTP/1, but that's no longer needed. OTOH, bundling still provides compression benefits.
On that front, there's a proposal I'm co-authoring to enable shared compression contexts as part of the H2 protocol, that will solve that
issue, and enable significantly better compression for small CSS and JS files.

(33 minutes)
---

# 3rd parties

???
And now, 3rd parties. A few things you can do to ease the pain of third parties

---

# Preconnect / Preload

???
Preconnect and preload enable you to speed up render blocking third parties

---

# Service Worker

???
Gives you back control over third parties. Enables you to know they are there, put SPOF protections in place, and otherwise defer or
terminate them in ways that weren't practical before.

---

# Long Tasks

???

Another recent API that can help you be aware of the impact of third party scripts on your site's runtime is the Long Tasks API. Be sure to
catch Nic Jansma's talk on that API on Friday morning. It's a tough slot, but totally worth your time.
---
class: bottom
background-image: url(media/ted.jpg)

# Script Manager

???
(Ted's face)

---

# Certificate frame

???

A new proposal called Certificate frame will enable us to avoid coalesce all Akamized third parties on the same connection as the first
party, avoiding bandwidth contention between them (because they would all be managed as part of a single sending queue).

---

# Client-side processing


???

Finally, for client-side processing, I don't have that much advice, other than the obvious "send less JS and CSS".
There are several best practices around critical CSS where a build step or automatic processing can help, but they tend to be hard to
configure and put in place.
For JS, there are many build step tools that can help (for example webpack), but they are not always something you can use, depending on
your workflow. Server-side rendering may also be applicable, but has to be carefully applied, as you need to make sure your site is not
frozen and non-responsive until the JS comes in.

So, unfortunately, no Ted-face here just yet. But this is definitly an area we're putting a lot of thought into.

---

# Take-aways
---

## Latency still dominates Web performance

---

## Multiple reasons for slow content

---

## We can improve things!

---
class: bottom
background-image: url(media/ted.jpg)

## Ion is awesome!

---

# Thank you!

.center[Yoav Weiss&nbsp;&nbsp;|&nbsp;&nbsp;   @yoavweiss]
![](media/Akamai-Logo-RGB.png)
---

# Questions?
.center[Yoav Weiss&nbsp;&nbsp;|&nbsp;&nbsp;   @yoavweiss]
![](media/Akamai-Logo-RGB.png)

---

### Credits
* https://www.flickr.com/photos/64252494@N07/7573429776 - Fast
* https://www.flickr.com/photos/eye-colored_mascara/16931514349 - stay weird
* https://www.flickr.com/photos/paigegphotography/4436216843 - dictionary
* https://www.flickr.com/photos/torkildr/3462607995 - servers
* https://www.flickr.com/photos/pheezy/3759117573 - flush
* https://www.flickr.com/photos/lorenjavier/5013332959 - bloat

---

### Credits
* https://www.flickr.com/photos/adactio/12674230513 - devices
* https://www.flickr.com/photos/jastrow/6347298190/ - taking back control
* https://www.flickr.com/photos/stooart/542546060/ - controls
* https://www.flickr.com/photos/medienzeitmaschine/5150471651 - IBM PC
* https://www.flickr.com/photos/ryanlavering/5180531131/ - surprise

</textarea><script type="text/javascript" src="remark-latest.min.js"></script></body></html>
