<!DOCTYPE html><html lang=en><head><meta http-equiv=Content-Type content="text/html; charset=UTF-8"><meta name=viewport content="width=device-width,initial-scale=1"><link rel="stylesheet" href="slidestyles.css" type="text/css" media="screen" charset="utf-8">

<!-- #ToDo: Change Title for the presentation -->
<title>Why is the web slow?</title>

</head>
    
<!-- #ToDo: Change 16:9 to 4:3 or vice versa, according to the current projector / screen -->
<body onload="var slideshow = remark.create({ratio: '16:9', navigation: {scroll: true, touch: true, click: false}, countIncrementalSlides: false, highlightLanguage: 'bash', highlightStyle: 'tomorrow-night-blue'});">

<!-- #ToDo: Write your slides in MarkDown here -->
<!--
# &#x276e;link&#x276F;
-->

<textarea id="source">

layout: true
class: center, middle
background-image: url(media/fast.jpg)

.low-right[@yoavweiss]
---

## Faster Bytes are not Enough!!
## Why is the web slow?
### (and what can we do about it)

<br>
<br>
.center[Yoav Weiss&nbsp;&nbsp;|&nbsp;&nbsp;   @yoavweiss]
.right[![](media/Akamai-Logo-RGB.png)]

???

Hi, I'm Yoav Weiss. I work at Akamai on making our CDN as well as browsers faster, and I'm here today to
talk about why the Web is inherently slow (which is a sad thing to say in a performance conference), and how can we make it faster despite that fact.
---
background-image: url(media/pc.jpg)

???
I’ve been in this industry for a while now. When I first started in the industry, I was working on a
web optimization proxy, as one does. My very first task was to build a
proprietary SPDY-like client-server protocol in order to be able to send
multiple requests and receive multiple responses at once over a single
connection. My second task was to integrate an image compression library
into the server. The image compression server was usually getting 2x
size reduction on the images. And network bandwidth at the time was a
serious issue, with top-notch mobile networks clocking at a cool
33kbps!! 
---
background-image: url(media/surprise.jpg)

???
But, we soon noticed something odd. Even with those awful
speeds, a 2x reduction in image sizes did not give us similar
performance improvements, unless they were deployed at the same time as
the SPDY-like client-server protocol (which could only be deployed for
users that actively installed the client, which was very few of them).
At first we were baffled by this. If we’re cutting down the
bytes we’re sending down by a factor of 2, why aren’t we seeing 2x
performance improvements???

---

# `time != `
# `(bytes / BW)`

???
The reason this was the case was that on the Web, the basic equations to calculate
how long "downloading a site" would take do not apply. We cannot treat the contents of a site as "bytes".
The contents of a Web site are discovered progressively, rendered progressively and often continue to download beyond the point in which
considers the site to be "loaded".
---

# Latency!

???
One of the reasons for the non-linearity that we saw was the impact of latency on the HTTP protocol.
The fancy networks at the time only delivered 33Kbps, but they also
did that with round trip times of 700ms, making the page’s loading
performance dominated by latency.

---

# HTTP/1.1 :shrug:
???
Requests in HTTP/1.0 and HTTP/1.1
were going out over multiple connections (2 connections per host for 1.1
at the time), but each connection could handle a single request at a
time, which meant that if you have to bring in 50 images to the page, it
would take you at a minimum 25 RTTs (even if all your images fit in a
single packet), or 17.5 seconds!!
---
class: bright

![](media/http1_1.svg)

---
class: bright

![](media/http1_2.svg)
---
class: bright

![](media/http1_3.svg)

???

But, that was a long time ago, right?
The industry has moved on, and those problems are all gone, *right*?

Well, we have advanced a lot in the last… almost 20 years. I’m really
old...
---

# Better *everything*!

???
We have significantly better networks, newer protocols such as HTTP2 and
QUIC, we have improved image formats, and we have significantly faster
computers.
Latency of cellular networks have decreased significantly (even when physical latency because you're fetching something from the other end
of the globe)
But we also have slower computers (pull out phone)

---
![](media/js_transfer_size.png)

???

And content on the web has evolved significantly over the last 20 years.
From simple text sites with images and an occasional JS based form
validation, it turned into a full fledged application development
platform, and one where you *download your entire app every time you
want to use it*.

---
.center[![](media/weird.jpg)]
???

And as a result the Web is still weird. Our performance bottlenecks are rarely where we think they are.
And despite high speed networks, we still see a *lot* of slow websites out there.

I'm of a firm belief that in order to understand how to make something work better and faster, you have to understand how it works in the
first place. And as a browser engineer, I really think everyone working on the web should know more about how browsers work.

---

# We'll discuss:
.center.big[
* How browsers work
* Web Performance issues
* Solutions
]
---
class: contain
background-image: url(media/dictionary.jpg)

???

But first things first, let’s start with some definitions.
Some people consider the words “web” and “internet” interchangeable. But
they have very distinct meanings.

---
class: contain
background-image: url(media/what_is_the_internet.png)

???

As our friends at Google tell us, the internet is a global computer
network providing a variety of information and communication facilities,
consisting of interconnected networks using standardized communication
protocols.

---

class: contain
background-image: url(media/what_is_the_web.png)

???
The web however, is “a network of fine threads constructed by a spider
from flui..” no
---
class: contain
background-image: url(media/world_wide_web.png)

???

The web is a network of interlinked documents, identified by their URLs,
and accessible via the internet. In other words, it’s a network of
documents, applications and content that is delivered over a network of
computers, the internet.

---
class: contain
background-image: url(media/what_is_a_browser.png)

???

A browser is a computer program that is used to display HTML content in order
to navigate the web.

---
.center[![](media/networks.jpg)]

???

So if we’d take the route of a metaphor, the internet is the roads, the
web is a network of interconnected attractions on those roads, URLs is
the road signs (stretching it, I know) and the browser is the vehicle we
can use to move from one location to the next.
---
class: bright

.inline[![](media/chrome.png)
![](media/safari.png)
![](media/firefox.png)
![](media/msedge.svg)
![](media/samsung.png)
]
???
So how do browsers load web pages?

---

##What's in a browser?
.center.big[
* Rendering engine
* Javascript engine
* Graphics layer
* Networking layer
* Image, video & audio decoders
]

???

Browsers are built out of multiple components that all play a part in loading of web pages: The networking layer fetches the required
resources from the web server, the rendering engine interprets them and finds more required resources, the javascript engine executes the
scripts which may often lead to more resources that needs to load, the decoders decode media formats into something that can be displayed to
the user and the graphics layer paints it all on the user's screen. It's a delicate and complex dance.
---
![](media/example.png)

???

So what happens when a user clicks on a link or types in an address in their browser's URL bar?

---
class: bright
![](media/dns.svg)

???

1 RTT for DNS
---

class: bright
![](media/dnstcp.svg)

???
Another RTT for TCP
---
class: bright
![](media/dnstcptls.svg)

???
2 more RTTs for TLS

---
.center[![](media/tcp_slow_start.png)]

???
TCP slow start is a mechanism that enables the server to gradually discover the connection's limits.
It also limits us in the amount of data that we can initially send.

---
class: bright
#Preloader
![](media/tokenization.svg)

???

Once the browser actually sends the HTML down to the browser, the browser starts processing it.
It tokenizes the HTML, and runs it through its preloader, in order to find out ASAP which resources it needs to load.
---

#Resource loading

???

That's because a web page is built out of multiple different file types: HTML, CSS, JS, fonts, images and  other media formats.
These resources are discovered by the preload scanner, by calculating style and by executing JS.
And the browser needs to discover those resources ASAP, because some of them are critical to render the page.
---
![](media/critical_rendering_path.png)

???
The browser needs the HTML in order to create the DOM - the Document Object Model.
Blocking scripts block the parsing of HTML until they are downloaded, so they block the DOM creation.
And CSS is required in order to create the CSS Object Model which together with the DOM is required to create the render tree, which is
required for page layout and paint to happen.

So before the user sees *anything* we need to download enough of the HTML and all the blocking CSS and JS.
---

# Priority

### HTML > CSS > JS > Fonts > Images

???
When downloading resources browsers need to decide which resources are more equal than others.
The basics is described here: HTML > CSS > JS > Fonts > Images, but in practice implementations have much more complex logic. Blocking scripts at the top are higher priority than
blocking scripts at the bottom. Async scripts are lower priority than blocking scripts. In-viewport images are higher priority than
out-of-viewport images. And this entire logic can vary between browsers and between browser versions.
---

![](media/tree_html_only.svg)

???
So, the browser starts out by just downloading the HTML. That's the only URL it's aware of.

---

![](media/tree_html_first_subresources.svg)

???

Then as the HTML is processed, CSS, images and scripts are discovered.
---

![](media/tree_html_second_subresources.svg)

???

But in many cases, CSS and scripts load other resources.
---

![](media/tree_html_third_subresources.svg)

???

Which then can load even more resources


CSS background images are declared in CSS, so discoverable only when the browser calculates the style.
Fonts were later added to the mix, and fonts are even more complicated. As they're only downloaded when the browser knows that they will be
used. So it has to calculate the style, and cross those style with the DOM to see if the resource is really needed.

---

## Performance issues

.center.big[
* Server side processing

* Discovery

* Bloat

* Out of control third parties

* Client side processing
]

???
Let's go over these problems in detail and see how we can reduce the
pain they cause.

---
background-image: url(media/servers.jpg)

# Server side processing

???

Historically, before the seminal web performance work by Steve Souders,
server side processing was considered *the* problem in web performance,
and it was what people were optimizing for. While Steve showed that
front end performance is responsible for 80% of the time people are
waiting for web pages, 20% is nothing to scoff at.
So server side performance is still very much something to be concerned
with.
At Akamai we see median server side performance times (or TTFB between the edge server and the origin server) of ~450ms, which
is a lot, especially if you consider the 3 seconds performance budget.

---
background-image: url(media/flush.jpg)
#Flush early

???
Early advice on server side performance had the advice to "flush early"
- you don't have to wait for your DB in order to let the browser start
  processing your most-probably-static HTML head and start fetching some
resources there. Only that this is often not the case - if your DB calls
can fail, they can impact your entire document (e.g. respond with a
completely different 404 page vs. your 200 page), and it's not always
trivial to "switch" pages on the user after an error has happened (and
without reloading the whole thing).

---

# Discovery
![](media/discovery.png)

???
Because of the way browsers work, because the browser has to download
the HTML, in order to find subresources to download, which in turn,
especially CSS and JS download more resources that they depend on, a lot
of the resources the browser needs to download are discovered at a late
phase.
Fonts and background images are notorious in that sense, since the
browser has to download all the CSS, and process the DOM in order to
find what's needed to be downloaded. But Javascript that loads resources
is just as guilty, as the entire script has to be processed before the
resource download can start.

---

background-image: url(media/dependency_tree.jpeg)
## Dependencies!

???
And this creates a dependency tree that's deep and contains many nodes,
each layer in that tree depends upon the downloading and processing of
the layer above it.
Now some resources are better at this than others: HTML and SVG can be
processed as they come, so a resource dependency in their first 100
bytes doesn't need to wait until the entire resource is downloaded
before the download can start.
But, CSS and JS are not the same. They have to be processed in their
entirety (and for CSS, all the blocking CSS has to be downloaed and
processed) before any resource download can start.
And that's not a bug, it's inherent to the way these formats work.

And there's inherent tension here between flattenning the dependency
tree which would help the browser load the page earlier, and the fact
that humans write websites, and for the humans, it's often better to
have smaller modules, which they can reason about without looking at the
larger picture.
So we have the old-fashioned CSS `@import`s and the newly fangled ES6 modules. They make
writing and managing code dependencies significantly easier. They also
make it harder for the browser, adding another layer to that dependency
tree.

---
background-image: url(media/bloat.jpg)

# Bloat

???

Traditionally we have been building web sites for desktops. 
Also traditinally, we compressed our images to avoid that kind of
experience
(autoplaying video of an image downloading slowly)
---
background-image: url(media/images_trend.png)

???
As broadband got better, we compressed our images less and less (HTTP
archive graph), and added more of them. And for our mobile version of
the site, we kept it slim.
But as more and more devices came to the market, each with its own
viewport dimensions, building a site version became simply not feasible. 
As a result, responsive design took over web design by storm, but in its
most naive version, we kept sending the same images we needed for our
high-end, high-bandwidth user experience to all devices.


---
background-image: url(media/devices.jpg)

# Client side processing

???

But the problem with bloat is not just the bandwidth waste. These
devices (points to phone) are not just downloading resources on an often
flakier network than our desktops. They also often have less CPU power
to process larger assets and less memory to deal with the consequences.

And even if we solved bloat for images, there's still a lot of CSS and
JS bloat to go around...
CSS frameworks are great to develop to, but often mean that you're
downloading way more CSS than you actually need *as a blocking
resource*.
JS frameworks and libraries often have a similar impact only that they
also tend to peg down the mobile device's CPU as their JS is being
processed (show timeline with CPU).


---
layout: true
class: center, middle
background-image: url(media/taking_back_control.jpg)

.low-right[@yoavweiss]
---

# Third parties

???
Everyone like to complain about third party performance...
It feels good to complain about that, cause by definition, it's someone
else that's doing a bad thing, and you're innocent :)

But when people talk about "third party performance problems" they usually conflate many different things.

---

## Render blocking

---

## SPOF

---

## Bandwidth contention with more critical resources
---

## Excessive downloads
---

## Periodic downloads
---

## Scroll/touch performance
---

## Hogging main thread
---

layout: true
class: center, middle
background-image: url(media/fast.jpg)

.low-right[@yoavweiss]
---
background-image: url(media/controls.jpg)
## What can we do about it?


---
background-image: url(media/servers.jpg)
# Server Side processing
---
background-image: url(media/servers.jpg)
# H2 push
---
background-image: url(media/servers.jpg)
# Early hints
---
background-image: url(media/discovery.png)
# Discovery

???
One solution for that tension is build scripts that take your
dependencies and mush them into a single blob.

That can be better, but:
* You have to parse the whole thing before anything is requested, so
  making a large unified resource can sometimes turn up to be slower.
* You're messing up the caching granularity

What are the solutions to that problem:
* H2 push can be used to push critical resources before the browser
  discovers them and realizes that it needs them.
* Link rel preload can be used to preload critical
---
background-image: url(media/discovery.png)
# H2 push
---
background-image: url(media/discovery.png)
# Preload

???

---

# Preload &gt; Push

* Cross origin
* Cache + cookies
* Content negotiation
* Load/error events

---

# Preload &lt; Push

.center[![](media/espn_bandwidth_pre_html.png)]

---

# Preload &lt; Push

.center[![](media/espn_bandwidth_pre_html_potential.png)]
???
What's the difference between preload and push?
(bring in slide from other talk)

Does push and preload solve all discoverability problems?
Not really, sometimes actual JS needs to run on the client in order to
figure out what resource the browser will need
(show code that randomally pick either img)

But as web developers, we can try to limit that variablity and make sure
we push and preload everything that's needed.

Aside on preload and priorities.
---
background-image: url(media/bloat.jpg)
# Bloat
---
background-image: url(media/bloat.jpg)
# Responsive images

???

The web performance community formed the Responsive Images Community
Group, and after a long while and numerous arguments, we managed to
convince browser vendors that we need to a way to specify multiple
images for browsers to download, based on their device dimensions,
design and the browser's environment.

And we came up with `<picture>`, srcset and sizes, specified them and
shipped them in browsers.

That, along with Client Hints, gives us the tools to manage image bloat
on the Web, and reduce it to a minimum, reduce its impact to a minimum.

So we made some progress with image bloat, but we're not there just yet.

There's also much progress on bloat in terms of better compression
algorithms, better image formats, etc.

---
background-image: url(media/bloat.jpg)
# Brotli
---
background-image: url(media/bloat.jpg)
# H2 shared dictionaries
---

# 3rd parties

---

# Client-side processing


???




Aside on shared dictionaries.


Critical CSS build step (or automatic processing) can help, but it may
be hard configure and get in place.

Build steps can help (webpack, etc), but they're not always
applicable...

---

# Take-aways
---

## Latency still dominates Web performance

---

## Multiple reasons why your content may be slow

---

## Ion and browser work underway to solve it

---

# Thank you!

.center[Yoav Weiss&nbsp;&nbsp;|&nbsp;&nbsp;   @yoavweiss]
![](media/Akamai-Logo-RGB.png)
---

# Questions?
.center[Yoav Weiss&nbsp;&nbsp;|&nbsp;&nbsp;   @yoavweiss]
![](media/Akamai-Logo-RGB.png)

---

### Credits
* https://www.flickr.com/photos/64252494@N07/7573429776 - Fast
* https://www.flickr.com/photos/eye-colored_mascara/16931514349 - stay weird
* https://www.flickr.com/photos/paigegphotography/4436216843 - dictionary
* https://www.flickr.com/photos/torkildr/3462607995 - servers
* https://www.flickr.com/photos/pheezy/3759117573 - flush
* https://www.flickr.com/photos/lorenjavier/5013332959 - bloat

---

### Credits
* https://www.flickr.com/photos/adactio/12674230513 - devices
* https://www.flickr.com/photos/jastrow/6347298190/ - taking back control
* https://www.flickr.com/photos/stooart/542546060/ - controls
* https://www.flickr.com/photos/medienzeitmaschine/5150471651 - IBM PC
* https://www.flickr.com/photos/ryanlavering/5180531131/ - surprise

</textarea><script type="text/javascript" src="remark-latest.min.js"></script></body></html>
