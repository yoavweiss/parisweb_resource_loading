<!DOCTYPE html><html lang=en><head><meta http-equiv=Content-Type content="text/html; charset=UTF-8"><meta name=viewport content="width=device-width,initial-scale=1"><link rel="stylesheet" href="slidestyles.css" type="text/css" media="screen" charset="utf-8">

<!-- #ToDo: Change Title for the presentation -->
<title>The Past, Present and Future of Resource Loading</title>

</head>
    
<!-- #ToDo: Change 16:9 to 4:3 or vice versa, according to the current projector / screen -->
<body onload="var slideshow = remark.create({ratio: '16:9', navigation: {scroll: true, touch: true, click: false}, countIncrementalSlides: false, highlightLanguage: 'bash', highlightStyle: 'tomorrow-night-blue'});">

<!-- #ToDo: Write your slides in MarkDown here -->
<!--
# &#x276e;link&#x276F;
-->

<textarea id="source">

layout: true
class: center, middle, bg-down
background-image: url(media/sketches/present.jpg)

---
class: no-bg

.absolute-up.no-bg.huge[
Past, Present and Future
of Resource Loading
]
.absolute-down.no-bg.huge[
]


.really-down.no-bg.big[
.center[Yoav Weiss&nbsp;&nbsp;|&nbsp;&nbsp;   @yoavweiss]
.absolute-bottom-left[![](media/Akamai-Logo-RGB.png)]
]

???

Hi, I'm Yoav Weiss. I work at Akamai on making our CDN as well as browsers faster, and I'm here today to
talk about why the Web is inherently slow (which is a sad thing to say in a performance conference), and how can we make it faster despite that fact.
---
layout: true
class: center, middle
background-image: url(media/sketches/present.jpg)

.low-right.no-bg[@yoavweiss]
---
background-image: url(media/pc.jpg)

???
I’ve been in this industry for a while now. When I first started in the industry, I was working on a
web optimization proxy, as one does. My very first task was to build a
proprietary SPDY-like client-server protocol in order to be able to send
multiple requests and receive multiple responses at once over a single
connection. My second task was to integrate an image compression library
into the server. The image compression server was usually getting 2x
size reduction on the images. And network bandwidth at the time was a
serious issue, with top-notch mobile networks clocking at a cool
33kbps!! 
---
background-image: url(media/surprise.jpg)

???
But, we soon noticed something odd. Even with those awful
speeds, a 2x reduction in image sizes did not give us similar
performance improvements, unless they were deployed at the same time as
the SPDY-like client-server protocol (which could only be deployed for
users that actively installed the client, which was very few of them).
At first we were baffled by this. If we’re cutting down the
bytes we’re sending down by a factor of 2, why aren’t we seeing 2x
performance improvements???

---

# `time != `
<br>
# `(bytes / BW)`

???
The reason this was the case was that on the Web, the basic equations to calculate
how long "downloading a site" would take do not apply. We cannot treat the contents of a site as "bytes".
The contents of a Web site are discovered progressively, rendered progressively and often continue to download beyond the point in which
considers the site to be "loaded".
---

# Latency!

???
One of the reasons for the non-linearity that we saw was the impact of latency on the HTTP protocol.
The fancy networks at the time only delivered 33Kbps, but they also
did that with round trip times of 700ms, making the page’s loading
performance dominated by latency.

But, that was a long time ago, right?
The industry has moved on, and those problems are all gone, *right*?

Well, we have advanced a lot in the last… almost 20 years. I’m really
old...


---

# Things got better!

???
We have significantly better networks, we have improved image formats, and we have significantly faster
computers.
Latency of cellular networks have decreased significantly (even when physical latency because you're fetching something from the other end
of the globe)
But we also have slower computers (pull out phone)

---
![](media/js_transfer_size.png)

???

And content on the web has evolved significantly over the last 20 years.
From simple text sites with images and an occasional JS based form
validation, it turned into a full fledged application development
platform, and one where you *download your entire app every time you
want to use it*.

---
???

And on that front, there's also a fundenmental difference between the web and previous internet protocols that preceded it.
Other protocols are often based on establishing a connection, downloading a resource and that's it.

The Web also started out that way. HTTP 0.9 was a very simple protocol that was used to download HTMLs.
But little by little, we added images. And styles. And scripts. And fonts. plugins. videos. audio.

And all these different resources now need to be downloaded together in order to create a web page. to create the experience we want to
deliver to our users.
---


???
So today I wanted to talk about how resources are loaded the web and all the problems with that which make this process inherently slow.
Then I want to go over a few of today's solutions to these problems. These solutions are significantly better than what we had in the good
old days of HTTP/1.1 and 33kbps bandwidth.
But they are not perfect. So I want to finish the tour by talking about tomorrow's solutions - all the things that are currently in the
works to improve resource loading on the web, and solve it once and for all (I hope).

Let's get started:

(6 minutes)

---

# Why is loading slow?

---
background-image: url(media/sketches/handshake.jpg)

--

# Connection establishment

???
When we start loading a page, once the user has clicked a link or wrote down a URL,
we start the connection establishment process. That the first step that's inherently latency bound.

---
background-image: url(media/sketches/handshake.jpg)
class: bright
.block[
![](media/dns.svg)
]

???

The browser has to translate the user visible hostname in the URL into an IP address, and therefore spends 1 RTT in that translation
process.

---

background-image: url(media/sketches/handshake.jpg)
class: bright
.block[
![](media/dnstcp.svg)
]

???
Then a TCP connection needs to be established between the browser and the server. That's a process called the 3 way handshake, which the
great Paul Irish once called "the what'sup protocol". The browser sends a packet to the server, letting it know that it wants to create a
connection. That's the first "whatsup". Then the server repeats the process, telling the client that it's ready to start a connection.
Only after that, we can start sending data on that TCP connection.

And that process, takes us another full RTT.
---
background-image: url(media/sketches/handshake.jpg)
class: bright
.block[
![](media/dnstcptls.svg)
]

???
TLS has its own version of the whatsup protocol, only it's more polite so here it's "Hello".
The browser says hello to the server, the server says hello to the browser, then because encryption is a bit more complex than sending data
in the clear, they need to exchange secrets and settle on an encryption key. I won't go into the details, but that usually takes up another
RTT, to bring us to 2 RTTs in total for TLS, and 4 RTTs for the entire connection establishment process.


---
background-image: url(media/sketches/server.jpg)

--

# Server side processing

???

Then, when the connection establishment parts are over, the browser can send out a request.
That's where the server side processing latency comes into play.

Historically, before the seminal web performance work by Steve Souders,
server side processing was considered *the* problem in web performance,
and it was what people were optimizing for. While Steve showed that
front end performance is responsible for 80% of the time people are
waiting for web pages, 20% is nothing to scoff at.
So server side performance is still very much something to be concerned
with.
At Akamai we see median server side performance times (or TTFB between the edge server and the origin server) of ~450ms, which
is a lot.
---
background-image: url(media/sketches/slowstart.jpg)

--

.up[
# Slow Start
]

???
Once the server has the HTML ready and starts sending it down, that's when we hit slow start.

---
background-image: url(media/sketches/slowstart.jpg)
.center[![](media/tcp_slow_start.png)]

???
TCP slow start is a mechanism that enables the server to gradually discover the connection's limits.
It also limits us in the amount of data that we can initially send.

Who here have heard of the 14KB rule, where all your critical resources have to fit into those first 14KB of your HTML?
Slow start (and an initial congestion window of 10 packets) are the reason for that rule. Everything beyond those initial 14KB will take
longer than an RTT to download, because the server is not sure it can send more than that without sending more than the network can handle.

---
background-image: url(media/sketches/discovery.jpg)

--
class: bright
.up[
# Discovery
]

---
background-image: url(media/sketches/discovery.jpg)
.block[
![](media/tokenization.svg)
]

???

Now, when the browser start receiving the HTML, it starts processing it , in order to find out ASAP which resources it needs to load.
It tokenizes the HTML in a streaming fashion, and runs it through its preloader to find all the resources that are based in the HTML.

But not all resources are in the HTML and can be discovered by the preload scanner. Some resources are only discovered after creating the
DOM tree, or after calculating styles or after executing scripts.

---

background-image: url(media/critical_path.jpg)

???
The browser needs the HTML in order to create the DOM - the Document Object Model.
Blocking scripts block the parsing of HTML until they are downloaded, so they block the DOM creation.
And CSS is required in order to create the CSS Object Model which together with the DOM is required to create the render tree, which is
required for page layout and paint to happen.

So before the user sees *anything* we need to download enough of the HTML and all the blocking CSS and JS.
---
background-image: url(media/sketches/discovery.jpg)

# Priority
<br>

### HTML > CSS > JS > Fonts > Images

???
When downloading resources browsers need to decide which resources are more equal than others.
The basics is described here: HTML > CSS > JS > Fonts > Images, but in practice implementations have much more complex logic. Blocking scripts at the top are higher priority than
blocking scripts at the bottom. Async scripts are lower priority than blocking scripts. In-viewport images are higher priority than
out-of-viewport images. And this entire logic can vary between browsers and between browser versions.
---

background-image: url(media/sketches/discovery.jpg)
.block[
![](media/tree_html_only.svg)
]

???
So, if we look at our resource download dependency tree, the browser starts out by just downloading the HTML. That's the only URL it's aware of.

---
background-image: url(media/sketches/discovery.jpg)

.block[
![](media/tree_html_first_subresources.svg)
]

???

Then as the HTML is processed, CSS, images and scripts are discovered.
---
background-image: url(media/sketches/discovery.jpg)

.block[
![](media/tree_html_second_subresources.svg)
]

???

But in many cases, CSS and scripts load other resources.
---
background-image: url(media/sketches/discovery.jpg)

.block[
![](media/tree_html_third_subresources.svg)
]

???

Which then can load even more resources


CSS background images are declared in CSS, so discoverable only when the browser calculates the style.
Fonts were later added to the mix, and fonts are even more complicated. As they're only downloaded when the browser knows that they will be
used. So it has to calculate the style, and cross those style with the DOM to see if the resource is really needed.

---
background-image: url(media/sketches/q.jpg)

--

# Queueing

---
background-image: url(media/sketches/q.jpg)

# HTTP/1.1 `¯\_(ツ)_/¯`
???
Requests in HTTP/1.0 and HTTP/1.1
were going out over multiple connections (2 connections per host for 1.1
at the time), but each connection could handle a single request at a
time, which meant that if you have to bring in 50 images to the page, it
would take you at a minimum 25 RTTs (even if all your images fit in a
single packet), or 17.5 seconds!!
---
background-image: url(media/sketches/q.jpg)
class: bright

.block[
![](media/http1_1.svg)
]

---
background-image: url(media/sketches/q.jpg)
class: bright

.block[
![](media/http1_2.svg)
]
---
background-image: url(media/sketches/q.jpg)
class: bright

.block[
![](media/http1_3.svg)
]

???

---
background-image: url(media/sketches/contention.jpg)

--

.up[
# Contention
]

???
The problem of bandwidth contention happens when we have multiple resources both downloaded at the same time, but where:
a) bandwidth is the bottleneck (which can often be the case in cellular networks)
b) High priority content is downloaded alongside lower priority content

In those cases, the lower priority content is slowing down the higher priority content (by competing with it on bandwidth).

In HTTP/1.1, we have bandwidth contention by definition, and browsers somehow mitigated that by delaying the sending of low priority
resources. In H2, it's no longer a problem between resources that are fetched on the same H2
connection, but is still very much a problem for resources fetched over other connections.

XXX: example of bandwidth contention between different resources of different priorities

---
background-image: url(media/sketches/bloat.jpg)

???

And finally, the seventh deadly sin of resource loading.

--

.up[
# Bloat
]

???

The bigger your resources, the longer it would take for them to download, the more vulnerable they will be to bad network events along the
way.
So resource size certainly still matters.


(15 minutes - 25 left)

---

.bullets[
* Connection establishment
* Server side processing
* Slow start
* Discovery
* Queueing
* Bandwidth contention
* Bloat
]


???
Let's go over these problems in detail and see how we can reduce the
pain they cause.

I'm purposefully leaving out of scope here third parties, which Andy covered in his talk.

---
layout: true
class: center, middle
background-image: url(media/sketches/handshake.jpg)

.low-right.no-bg[@yoavweiss]
---

???

So we've seen that connection establishment is something that takes a large number of RTTs and is inherently latency bound. How can we get
rid of it?

---

# Preconnect
<br>
.code-size[
### `<link rel=preconnect>`
]

???

One potential answer, at least for everything other than the navigation request, is preconnect.
You can use `<link rel=preconncet>` hints in either your markup or your HTML in order to make sure the browser connects ahead of time to
every resource that is a potential bottleneck and which lives on a host different from the main host.

XXX: Add an example of such requests?
---

# Connection coalescing

???
Another potential answer for those kind of requests is to use connection coalescing to do the work for you.
The HTTP/2 protocol defines that all the hostnames that are covered by a single certificate are to be coalesced together, given that they
translate to the same IP.

So, if you serve content from multiple domains that you own, your performance will likely benefit significantly from making sure that all
these hosts are convered by the same certificate (so mentioned in the certificate's Server Alternate Name extension). You also have to make
sure that all these hosts translate to the same IP. Then you bypass most of the connection establishment phase, and just reuse the old
connection.

Why did I say most? Because DNS is still required, in order to *know* that these hosts all translate to the same IP, and to provide some
safety guarantees to the feature.

---

# QUIC

???

Another approach is to abandon protocols that require a connection establishment all together!
There are few options there, but the most radical (and well-known one) is QUIC, where the protocol "remembers" the old connection between
the browser and the server, and reuses the exchanged keys in future connections. QUIC is a protocol that replaces TCP and TLS completely and
as such only has a single RTT handshake the first time a user hits it, and a 0-RTT handshake the next time around. 0-RTT means that the
client can immediately start sending a request to the server, and the server uses information stored in those packets from the previous
connection in order to establish the new one immediately.

One major problem with that today is that since QUIC is a completely different protocol, browsers don't know which server supports it, so
they rely on openning a regular TCP+TLS connection, send a request, and get an `Alt-Svc` response which tells them the server supports this
new protocol.
So we pay all those extra RTTs today on the initial connection. That's fine if you're Google and most of your users are repeat visits. It's
less fine if you have many first visitors on your site.

From a deployment perspective, Google's QUIC is currently a non-standard protocol, which is fast evolving and only supported by Chromium
based browsers.

---

# TCP Fast Open + TLS 1.3

???

We've seen QUIC, which is a totally new protocol. A more conservative options to get rid of the connection establishment RTTs for repeat
visitors is using TCP Fast Open and TLS/1.3

These extensions to the existing protocols enable a similar option that makes sure that for repeat visits, the server can recreate the
connection based on past info, and can save precious RTTs in that case.

They are, in a sense simpler to deploy and standard. At the same time, there are some networks which react badly to TFO usage and penalize
users which try to use it (and they misunderstand the packets and consider them a security threat :/).

Supporting browsers and network stacks (mostly Apple nowadays) have put in place various mitigations to prevent that from happening often.

TLS/1.3 support is starting to role out in browsers. Chrome will start supporting it starting from version 65 (mid March). I'm guessing
support from other browsers won't be too far behind.

XXX: support in Firefox? Apple? Microsoft?

---

# What's in the future?

???
We mentioned a couple of disadvantages for QUIC: It is non-standard so rapidly evolving, and it's negotiation mechanism takes a long
while at the first visit, which renders its benefits mute for that case.

These two problems have solutions underway:
---

# IETF QUIC

???
The IETF QUIC WG is working its way to standardizing the protocol, and plans to finish that by end of 2018.

---

# Alt-SVC over DNS

???
At the same time, there's an ongoing proposal for an Alt-Svc extension for DNS, that will enable servers to declare QUIC support already at
the DNS phase, and save many precious RTTs as browsers will "speak" QUIC directly to the servers instead of trying TCP+TLS first.

---
layout: true
class: center, middle
background-image: url(media/sketches/server.jpg)

.low-right.no-bg[@yoavweiss]
---

# Server side processing

???

Server side processing is the next hurdle to loading, mainly for the HTML, which is often dynamic, and which delivery is critical for the
loading of everything else.

It's hard to give you generic advice on how to speed up your server side processing. You can instrument and figure out which operations take
time, and try to speed them up, but at the end of the day, executing complex, scalable applications to create dynamic HTML pages can take
time.

It also doesn't help that the default runtime of many high level languages such as Python and Ruby is not extremely fast be default.

But even if our backend times can be slow at times, that doesn't mean we can't put this waiting time to good use.
---
background-image: url(media/flush.jpg)
#Flush early

???
Early advice on server side performance had the advice to "flush early"
- you don't have to wait for your DB in order to let the browser start
  processing your most-probably-static HTML head and start fetching some
resources there. Only that this is often not the case - if your DB calls
can fail, they can impact your entire document (e.g. respond with a
completely different 404 page vs. your 200 page), and it's not always
trivial to "switch" pages on the user after an error has happened (and
without reloading the whole thing).

---

# H2 push

???

Server push is a feature of the HTTP/2 protocol, which enables us to send down resources before the browser requested them.
As such, it enables us to put the network connection to good use while we're waiting for the HTML to be generated by the server.
---

## Using the wait time


.wide-img.center[![](media/espn_bandwidth_pre_html.png)]

???

In other words it enables us to turn this bandwidth graph, into this.

---

## Using the wait time

.wide-img.center[![](media/espn_bandwidth_pre_html_potential.png)]
---
.block[
![](media/page_loading_nopush.svg)
]

???
300 ms think time
100 ms RTT
98KB CSS+JS
120KB HTML

---
.block[
![](media/page_loading_push.svg)
]

???

Another advantage of this is that it enables us to warn up the TCP connection while waiting for the HTML, which can significantly reduce the
amount of RTTs and time it takes us to send down the HTML and the critical CSS and JS resources.

---
background-image: url(media/h2_push_tough.png)

???

---

# What's in the future?
---

# Cache Digests
???

One of the main complaints about H2 push, beyond bugs in various implementations is that there's no standard way for the server to know
what's in the browser's cache, and therefore, no way to not push resources that are already there.

Cache Digests is a new proposal that will enable a browser to give a condensed list to the server, letting it know of everything it has in
its cache. The server can then know which resources it doesn't need to push, without jumping through hoops and trying to guess that info.

---

# Early hints

???
A recent standard also suggests to define a way for us to be able to send headers to the browser before the response is complete,
which can enable us to send instructions to the browser to preconnect hosts or preload resources before the response is ready, which can be
helpful to speed up blocking third parties, especially if they are lately discovered.

(23 minutes, 17 left)
---
layout: true
class: center, middle
background-image: url(media/sketches/discovery.jpg)

.low-right.no-bg[@yoavweiss]
---

.block.up[
# Discovery
]

???
Because of the way browsers work, because the browser has to download
the HTML, in order to find subresources to download, which in turn,
especially CSS and JS download more resources that they depend on, a lot
of the resources the browser needs to download are discovered at a late
phase.
Fonts and background images are notorious in that sense, since the
browser has to download all the CSS, and process the DOM in order to
find what's needed to be downloaded. But Javascript that loads resources
is just as guilty, as the entire script has to be processed before the
resource download can start.

---

background-image: url(media/dependency_tree.jpeg)

.block.down[
## Dependencies!
]

???
And this creates a dependency tree that's deep and contains many nodes,
each layer in that tree depends upon the downloading and processing of
the layer above it.
Now some resources are better at this than others: HTML and SVG can be
processed as they come, so a resource dependency in their first 100
bytes doesn't need to wait until the entire resource is downloaded
before the download can start.
But, CSS and JS are not the same. They have to be processed in their
entirety (and for CSS, all the blocking CSS has to be downloaed and
processed) before any resource download can start.
And that's not a bug, it's inherent to the way these formats work.

And there's inherent tension here between flattenning the dependency
tree which would help the browser load the page earlier, and the fact
that humans write websites, and for the humans, it's often better to
have smaller modules, which they can reason about without looking at the
larger picture.
So we have the old-fashioned CSS `@import`s and the newly fangled ES6 modules. They make
writing and managing code dependencies significantly easier. They also
make it harder for the browser, adding another layer to that dependency
tree.

---
![](media/discovery.png)

???

And you can see that the bandwidth graph for that particular site has a lot of holes in it,
during which the browser wasn't downloading anything.

OK, so this is a real issue. What are the solutions to that problem?
---

# Preconnect and H2 push

???

We already discussed preconnect and H2 push in the context of connection establishment and server think time, but we can also think of them as helping
discovery, by eliminating latency from the delivery of these resources, and making sure that when the browser discovers it needs them, the
connections are ready to go, or the resources are already there in the cache.
---

# Preload
<br>
.code-size[
## `<link rel=preload>`
]
???

Preload is another recent feature that I've been working on. It enables you to tell the browser to kick off the download of a certain
resource, without defining the exact point where the resource would be used. So, it decouples download from execution, and enables you to
"flatten" the dependency tree by preloading late-discovered resources ahead of time.

---

# Preload &gt; Push

.center.big[
* Cross origin
* Cache + cookies
* Content negotiation
* Load/error events
]

???
If we compare preload to push, it has multiple advantages: it can work for third party resources where H2 push is essentially meant for
resources under the first party's control. It also takes the cache state into account, as well as the cookies on the client side, where push
cannot do that. It can be used in content-negotiation scenarios, so for example can be used to fetch webp images only to browsers that
supprot them, using `Accept` headers. On top of that, it has load and error events, enabling you to build complex JS based loading logic
using preload.

---

# Preload &lt; Push

.wide-img.center[![](media/espn_bandwidth_pre_html_potential.png)]
???

What's push biggest advantage? The one we've seen before: it can be used to "fill up" the idle times before the browser the browser received
the HTML, resulting in a significant boost to the time the user first sees content on the screen.

---

.left[
```javascript
let img = new Image();
if (Math.random() > 0.5) {
    img.src = "image1.jpg";
} else {
    img.src = "image2.jpg";
}
```
]

???
Does push and preload solve all discoverability problems?
Not really, sometimes actual JS needs to run on the client in order to
figure out what resource the browser will need

But as web developers, we can try to limit that variablity and make sure
we push and preload everything we can that's needed.

---

# Priorities
<br>
.code-size[
##`<link rel=preload as=script>`
]

???
Just a short aside: preload enables the browser to assign the right priorities to the resource in question by declaring the type of resource
using the `as` attribute. I'm also working on a different specification that will enable us to define explicit priorities for certain
resources, and change their default priorities.

---

# What's in the future?

---

# Priority Hints

???

We talked about resource priorities and how the browser determines them based on heuristics such as the resource type and location in the
document. Those heuristics work great most of the time, but sometimes they miss, and exagerate either giving a resource too high of a
priority or too low one. Priority Hints which is still very much a work in progress, will enable developers to define the priorities of
certain resources, and help the browser do the right thing.

(29 minutes, 11 left)

---
layout: true
class: center, middle
background-image: url(media/sketches/q.jpg)

.low-right.no-bg[@yoavweiss]
---

# Queueing

???

Now queueing. We talked about the perils of HTTP/1.1 and how each connection there could only send a single request at a time.

---

# HTTP2 to the rescue!

???
Well, H2 solves that problem.
With H2 you can send multiple streams (each one corresponding to a request-response pair) on a single connection.
So you can send a very large number of requests in parallel, and have the server serve you the one that is ready.
That's a huge difference from the way connection handling was done in H1.

Aside on single line queues in airport security vs. multi passanger lines

---
???
Diagram of H2 vs. H1

---

# H2 priorities

???

Since H2 changes many things from H1, it also had to rethink priorities. Before H2 prioritization was simply done by not sending certain
requests. When H2 came around, it added a prioritization scheme, so that the browser sends all the requests to the server, and the server
orders the responses based on their priorities.

That breaks when you have many H2 servers, when you're working off of many domains, as the result of that is contention. each server sends
back the responses that matter the most to the server, but if their most important resource is a low-priority one, it will contend with high
priority resources from a different server.

---

# H2 doesn't fix everything

???

While H2 is significantly better than H1, it's not quite perfect.
Because the priority queues are managed in user space, it's hard for the server to be
very reactive to priority changes, which often leads to low-priority resources sent before high-priority ones.

Another problem is TCP losses. Since we use a single TCP connection and TCP is a reliable protocol, any loss means that all H2 streams now
have to wait for that TCP pakcet to get retransmitted. That's a reincarnation of the Head-of-line blocking problem, in a different way.

Finally, since there's no shared compression context between resources - we cannot compress multiple files together as if they were one - it
means that it's still beneficial to bundle resources together, even when H2 is around.

---

# What's in the future?
---


# QUIC and IETF QUIC

???

One thing we already discussed and that is planned to solve these issues is QUIC and its standard friend IETF QUIC.
Moving the entire stack to user-space would mean that implementations would have tighter control over what they are sending and when they
are sending it.

Also, the QUIC protocol handles packet losses differently, meaning that a lost packet will only impact the resources that must be impacted
(because they wait for the data), but other resources can continue to be read despite it. No more Head-of-line!

---

# H2 compression dictionaries

???

There is also a current proposal for creating H2 level compression that will be able to compress multiple resources as if they were a single
resource. There are currently some security hurdles to making that a reality.
---

# Web Packaging


???
Another alternative to bundling is better bundling!
The Web Packaging proposal will enable to have a file format that bundles multiple resources together (so they can be compressed together)
while at the same time enable to read and process each one of them individually. Since JS/CSS processing must be done on an entire file,
that would mean that processing can happen in parallel to downloading instead of happening only once the full file is downloaded.
---
layout: true
class: center, middle
background-image: url(media/sketches/slowstart_contention.jpg)

.low-right.no-bg[@yoavweiss]
---

.up[
## Slow Start & Contention
]


???

XXX - add about slow start and how connection coalescing is the only solution there
otherwise, 14KB.
Emphasize that slow start is relevant for every new connection established


---

.up[
# Connection Coalescing
]

---

.up[
# Certificate frame
]

???

A new proposal called Certificate frame will enable us to avoid coalesce all Akamized third parties on the same connection as the first
party, avoiding bandwidth contention between them (because they would all be managed as part of a single sending queue).

---

.up[
# Origin signed resources
]

???

---
layout: true
class: center, middle
background-image: url(media/sketches/bloat.jpg)

.low-right.no-bg[@yoavweiss]
---

# Bloat

---

# Brotli

---

# Custom dictionaries

---

# Compression API
---
layout: true
class: center, middle
background-image: url(media/sketches/present.jpg)

.low-right.no-bg[@yoavweiss]
---

# Take-aways
---

## Latency still dominates resource loading on the web

---

## Multiple reasons for slow content

---

## We can improve things!

---

layout: true
class: center, middle
background-image: url(media/sketches/present.jpg)
---

# Thank you!

.no-bg.big[
.down.center[Yoav Weiss&nbsp;&nbsp;|&nbsp;&nbsp;   @yoavweiss]
![](media/Akamai-Logo-RGB.png)
]
---

# Questions?
.no-bg.big[
.down.center[Yoav Weiss&nbsp;&nbsp;|&nbsp;&nbsp;   @yoavweiss]
![](media/Akamai-Logo-RGB.png)
]

---

### Credits

<br>
<br>
Huge thanks to Yaara Weiss for her awesome illustrations!
<br>
<br>
.bullets.small[
* https://www.flickr.com/photos/medienzeitmaschine/5150471651 - IBM PC
* https://www.flickr.com/photos/ryanlavering/5180531131/ - surprise
* https://www.flickr.com/photos/pheezy/3759117573 - flush
]

???
TODO:
* Future images, "past present future image", summary image, etc
* 

</textarea><script type="text/javascript" src="remark-latest.min.js"></script></body></html>
